{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717333df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## install required packages\n",
    "# !pip install category_encoders --quiet\n",
    "# !pip install xgboost --quiet\n",
    "# !pip install optuna --quiet\n",
    "\n",
    "## import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from feature_eng import feature_engineering\n",
    "from sklearn.metrics import  root_mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "from feature_eng import feature_engineering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing import Preprocessor,train_x, test_x, train_y, test_y\n",
    "\n",
    "\n",
    "## Use Hyperparameter optimization with Optuna and 10-fold cross-validation - XGBRegressor\n",
    "\n",
    "# -------------------------\n",
    "# Optuna objective\n",
    "# -------------------------\n",
    "def objective(trial):\n",
    "\n",
    "    preprocessor = Preprocessor(\n",
    "        target_smoothing=trial.suggest_float(\n",
    "            \"target_smoothing\", 0.3, 5.0, log=True\n",
    "        ),\n",
    "        min_samples_leaf=trial.suggest_int(\n",
    "            \"te_min_samples_leaf\", 20, 80\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Define XGBRegressor hyperparameters to tune\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 150, 300),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": 1,\n",
    "        \"tree_method\": \"hist\"\n",
    "    }\n",
    "\n",
    "    # Create the model\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    pipeline = make_pipeline(preprocessor, model)\n",
    "\n",
    "    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        pipeline,\n",
    "        train_x,\n",
    "        train_y,\n",
    "        cv=cv,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        n_jobs=-1  # parallelize only at CV level\n",
    "    )\n",
    "\n",
    "    return -scores.mean()\n",
    "\n",
    "# -------------------------\n",
    "# Run Optuna study\n",
    "# -------------------------\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=2),\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# -------------------------\n",
    "# Print best hyperparameters\n",
    "# -------------------------\n",
    "print(\"Best CV RMSE:\", study.best_value)\n",
    "print(\"Best parameters:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "best = study.best_params\n",
    "\n",
    "# -------------------------\n",
    "# Build final leak-proof pipeline using **params\n",
    "# -------------------------\n",
    "final_preprocessor = Preprocessor(\n",
    "    target_smoothing=best[\"target_smoothing\"],\n",
    "    min_samples_leaf=best[\"te_min_samples_leaf\"],\n",
    ")\n",
    "\n",
    "final_params = {\n",
    "    \"n_estimators\": best[\"n_estimators\"],\n",
    "    \"max_depth\": best[\"max_depth\"],\n",
    "    \"learning_rate\": best[\"learning_rate\"],\n",
    "    \"subsample\": best[\"subsample\"],\n",
    "    \"colsample_bytree\": best[\"colsample_bytree\"],\n",
    "    \"reg_alpha\": best[\"reg_alpha\"],\n",
    "    \"reg_lambda\": best[\"reg_lambda\"],\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "    \"tree_method\": \"hist\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Final model\n",
    "final_model = XGBRegressor(**final_params)\n",
    "\n",
    "final_pipeline = make_pipeline(\n",
    "    final_preprocessor,\n",
    "    final_model\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Train final model\n",
    "# -------------------------\n",
    "final_pipeline.fit(train_x, train_y)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_preds = final_pipeline.predict(test_x)\n",
    "test_rmse = root_mean_squared_error(test_y, test_preds)\n",
    "print(\"Final test RMSE:\", test_rmse)\n",
    "\n",
    "# Save final model\n",
    "joblib.dump(\n",
    "    final_pipeline,\n",
    "    f\"xgb_pipeline_{pd.Timestamp.today().strftime('%Y-%m-%d-%H-%M-%S')}.joblib\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Final XGB model saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
