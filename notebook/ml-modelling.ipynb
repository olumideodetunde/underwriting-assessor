{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Modelling"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's attempt to build a frequency vs severity model using both GLM and GBM. Remember that technically `premium = frequency * severity`\n",
    "- Bonus: see if along the way we can use the variable importance to identify key rating factors for both frequency and severity that can be used for risk classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 001: Create the dataset and split dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.dataset import Dataset\n",
    "\n",
    "\n",
    "insurance_initiation_variables_path = \"../data/input/exp/Insurance_Initiation_Variables.csv\"\n",
    "claims_variables_path = \"../data/input/exp/sample_type_claim.csv\"\n",
    "\n",
    "claim_grouping_columns = ['ID', 'Cost_claims_year']\n",
    "claim_aggregation_column = 'Cost_claims_by_type'\n",
    "merging_columns = ['ID', 'Cost_claims_year']\n",
    "\n",
    "dataset =  (Dataset(data_path=insurance_initiation_variables_path,\n",
    "                              claims_path=claims_variables_path)\n",
    "                      .group_claims(grouping_columns=claim_grouping_columns,aggregation_column=claim_aggregation_column)\n",
    "                      .create_dataset(merge_columns=merging_columns)\n",
    "                     )\n",
    "trainset, testset = dataset.split_dataset(test_ratio=0.2, to_shuffle=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T19:34:51.936525Z",
     "start_time": "2025-09-08T19:34:51.933596Z"
    }
   },
   "cell_type": "markdown",
   "source": "## 002: Engineer relevant features"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.feature import  main as feature_main\n",
    "\n",
    "features_trainset = feature_main(trainset)\n",
    "features_testset = feature_main(testset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 003: Frequency modelling - Poisson regression\n",
    "\n",
    "The response variable is the number of claims dubbed `claims_frequency` in the dataset. Let's check a few assumptions before we fit a Poisson regression model:\n",
    "\n",
    "1. Distribution of the response variable\n",
    "2. Equidispersion: the mean and variance of the response variable should be roughly equal"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "cols = ['claims_frequency']\n",
    "bins = 10\n",
    "fig, axes = plt.subplots(1, len(cols), figsize=(18, 3))\n",
    "if len(cols) == 1:\n",
    "    axes = [axes]\n",
    "for i, col in enumerate(cols):\n",
    "    sns.histplot(data=features_trainset, x=col, bins=bins, kde=False, ax=axes[i])\n",
    "    axes[i].set_title(f\"Distribution of {col}\")\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mean_claims_frequency = features_trainset['claims_frequency'].mean()\n",
    "var_claims_frequency = features_trainset['claims_frequency'].var()\n",
    "print(f\"Mean of claims_frequency: {mean_claims_frequency}\")\n",
    "print(f\"Variance of claims_frequency: {var_claims_frequency}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- From the both the histogram and the mean-variance comparison, we can see that the claims frequency is unimodal and rightly skewed, the mean and variance are not roughly equal (variance is smaller than the mean). This indicates that the data is underdispersed. So lets try to fit a poisson regression model and see how it performs."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "features_trainset.columns",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_variables = ['Car_age_years', 'Type_risk', 'claims_frequency', 'Area', 'Value_vehicle', 'Distribution_channel', 'Cylinder_capacity', 'Length', 'N_doors']\n",
    "features_trainset[training_variables].corr()['claims_frequency'].drop('claims_frequency').plot(kind='barh', title='Correlations with Claims Frequency')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Fit the poisson regression model using statsmodel formula api\n",
    "from statsmodels.formula.api import poisson\n",
    "poisson_model = poisson('claims_frequency ~ Value_vehicle', data=features_trainset, methods='bfgs')\n",
    "result_1 = poisson_model.fit()\n",
    "print(result_1.summary())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " #Fit the poisson model using discrete Generalized Poisson model from statsmodels\n",
    "from statsmodels.discrete.discrete_model import GeneralizedPoisson\n",
    "generalised_poisson_model = GeneralizedPoisson(endog=features_trainset['claims_frequency'], exog=features_trainset[['Driver_age_years', 'Driver_experience_years', 'Car_age_years', 'power_to_weight', 'Type_risk']], p=2).fit(method='nm')\n",
    "print(generalised_poisson_model.summary())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#TODO: interpret the model results and look into evaluation metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred = result_1.predict(features_testset)\n",
    "y_true = features_testset['claims_frequency'].fillna(0)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"Poisson Model Evaluation on Test Set:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"MAE (Mean Absolute Error): {mae:.4f}\")\n",
    "print(f\"RMSE (Root Mean Square Error): {rmse:.4f}\")\n",
    "print(f\"RÂ² (R-squared): {r2:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
