{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Modelling",
   "id": "e46039f440e1bc89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Let's attempt to build a frequency vs severity model using both GLM and GBM. Remember that technically `premium = frequency * severity`\n",
    "- Bonus: see if along the way we can use the variable importance to identify key rating factors for both frequency and severity that can be used for risk classification"
   ],
   "id": "c32197f8b9577e32"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 001: Create the dataset and split dataset",
   "id": "9725c49a1631ccc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.dataset import Dataset\n",
    "\n",
    "\n",
    "insurance_initiation_variables_path = \"../data/input/exp/Insurance_Initiation_Variables.csv\"\n",
    "claims_variables_path = \"../data/input/exp/sample_type_claim.csv\"\n",
    "\n",
    "claim_grouping_columns = ['ID', 'Cost_claims_year']\n",
    "claim_aggregation_column = 'Cost_claims_by_type'\n",
    "merging_columns = ['ID', 'Cost_claims_year']\n",
    "\n",
    "dataset =  (Dataset(data_path=insurance_initiation_variables_path,\n",
    "                              claims_path=claims_variables_path)\n",
    "                      .group_claims(grouping_columns=claim_grouping_columns,aggregation_column=claim_aggregation_column)\n",
    "                      .create_dataset(merge_columns=merging_columns)\n",
    "                     )\n",
    "trainset, testset = dataset.split_dataset(test_ratio=0.2, to_shuffle=False)"
   ],
   "id": "7a3f1898b8d877bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 002: Engineer relevant features",
   "id": "b95a7817968c222d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.feature import  main as feature_main\n",
    "\n",
    "features_trainset = feature_main(trainset)\n",
    "features_testset = feature_main(testset)"
   ],
   "id": "7a22ff8fa3abcfc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fill nulls in claims_frequency with 0 - missed during dataset creation step, to be added later\n",
    "features_trainset.loc[:, 'claims_frequency'] = features_trainset['claims_frequency'].fillna(0)\n",
    "features_testset.loc[:, 'claims_frequency'] = features_testset['claims_frequency'].fillna(0)"
   ],
   "id": "62e263d8057bd226"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 003: Frequency modelling - Poisson regression\n",
    "\n",
    "The response variable is the number of claims dubbed `claims_frequency` in the dataset. It is important to note that Claims frequency is actually a rate (i.e. it is the claims frequency for a year for each policy-holder). However, since the exposure is constant for all policies in this dataset (1 year), it is an implicit rate.\n",
    "\n",
    "\n",
    "Let's check a few assumptions before we fit a Poisson regression model:\n",
    "1. Distribution of the response variable\n",
    "2. Equidispersion: the mean and variance of the response variable should be roughly equal"
   ],
   "id": "15ec8abc2192221d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "ax0.set_title('Claims Frequency Distribution')\n",
    "_ = features_trainset['claims_frequency'].hist(bins=4, log=True, ax=ax0)\n",
    "\n",
    "print(\n",
    "    \"Average claims frequency: {}\".format(\n",
    "        np.average(features_trainset['claims_frequency'])\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Fraction of claims frequency that is Zero {0:.2%}\".format(\n",
    "        features_trainset[features_trainset['claims_frequency']==0].__len__() / features_trainset['claims_frequency'].__len__()\n",
    "    )\n",
    ")"
   ],
   "id": "fccad599705c562f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mean_claims_frequency = features_trainset['claims_frequency'].mean()\n",
    "var_claims_frequency = features_trainset['claims_frequency'].var()\n",
    "print(f\"Mean of claims_frequency: {mean_claims_frequency :.4f}\")\n",
    "print(f\"Variance of claims_frequency: {var_claims_frequency:.4f}\")"
   ],
   "id": "535be0346e47222c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- From the both the histogram and the mean-variance comparison, we can see that the claims frequency is unimodal and rightly skewed, the mean and variance after filling nulls with 0 appear to be reasonably close for this dataset. We can therefore proceed to fit a Poisson regression model.",
   "id": "560bb6b71f815e1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_variables = ['Car_age_years', 'Type_risk', 'Area', 'Value_vehicle', 'Distribution_channel', 'Cylinder_capacity']\n",
    "target = ['claims_frequency']"
   ],
   "id": "a3753f3baa52c31d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_poisson_deviance,mean_squared_error\n",
    "\n",
    "def model_evaluation_metrics(estimator, df_test, target_variable=target, training_variables=training_variables):\n",
    "    \"\"\"Score an estimator on the test set.\"\"\"\n",
    "    y_pred = estimator.predict(df_test[training_variables])\n",
    "\n",
    "    print(\n",
    "        \"MSE: %.3f\"\n",
    "        % mean_squared_error(\n",
    "            df_test[target], y_pred,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"MAE: %.3f\"\n",
    "        % mean_absolute_error(\n",
    "            df_test[target], y_pred\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Ignore non-positive predictions, as they are invalid for\n",
    "    # the Poisson deviance.\n",
    "    mask = y_pred > 0\n",
    "    if (~mask).any():\n",
    "        n_masked, n_samples = (~mask).sum(), mask.shape[0]\n",
    "        print(\n",
    "            \"WARNING: Estimator yields invalid, non-positive predictions \"\n",
    "            f\" for {n_masked} samples out of {n_samples}. These predictions \"\n",
    "            \"are ignored when computing the Poisson deviance.\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"mean Poisson deviance: %.3f\"\n",
    "        % mean_poisson_deviance(\n",
    "            df_test[target][mask],\n",
    "            y_pred[mask],\n",
    "        )\n",
    "    )\n"
   ],
   "id": "cee61893a4d04cfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 1 - Baseline Model, Just predicting the mean",
   "id": "13ff805c0a11471b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "dummy_regressor = DummyRegressor(strategy=\"mean\")\n",
    "baseline_model = dummy_regressor.fit(features_trainset[training_variables], features_trainset[target])\n",
    "print(\"Constant mean frequency evaluation:\")\n",
    "model_evaluation_metrics(estimator=baseline_model, df_test=features_testset, target_variable=target, training_variables=training_variables)"
   ],
   "id": "f5b11aaa26eb0120"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 2 - Ridge Regression",
   "id": "518012b6c3233cff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_glm = Ridge(alpha=1)\n",
    "ridge_model = ridge_glm.fit(features_trainset[training_variables], features_trainset[target])\n",
    "print(\"Ridge regression evaluation:\")\n",
    "model_evaluation_metrics(estimator=ridge_model, df_test=features_testset, target_variable=target, training_variables=training_variables)"
   ],
   "id": "1dfbd2b1f3cb99ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 3 - Poisson Regression",
   "id": "97423099565f9906"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "poisson_regressor = PoissonRegressor(alpha=1e-12, solver='newton-cholesky', max_iter=300)\n",
    "poisson_model = poisson_regressor.fit(features_trainset[training_variables], features_trainset[target].values.ravel())\n",
    "print(\"Poisson regression evaluation:\")\n",
    "model_evaluation_metrics(estimator=poisson_model, df_test=features_testset, target_variable=target, training_variables=training_variables)"
   ],
   "id": "38746389a273d7d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 4 - Gradient Boosting Machine\n",
   "id": "d20ccabc6b1ff1f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "gbm_regressor = HistGradientBoostingRegressor(loss='poisson', max_leaf_nodes=128)\n",
    "gbm_model = gbm_regressor.fit(features_trainset[training_variables], features_trainset['claims_frequency'])\n",
    "print(\"GBM regression evaluation:\")\n",
    "model_evaluation_metrics(estimator=gbm_model, df_test=features_testset, target_variable=target, training_variables=training_variables)"
   ],
   "id": "c1c51e9508a8cbef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#TODO: Severity Modelling",
   "id": "af5d40bdb3055002"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
