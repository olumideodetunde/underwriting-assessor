{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Frequency Modelling",
   "id": "653a0d2d5b8e0dfd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "- Let's attempt to build a frequency vs severity model using both GLM and GBM. Remember that technically `premium = frequency * severity`\n",
    "- Bonus: see if along the way we can use the variable importance to identify key rating factors for both frequency and severity that can be used for risk classification"
   ],
   "id": "71c84a9cd9399b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup",
   "id": "3ac3b5c7cc59ed7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "b364aae7431f3049",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Prepare Dataset\n",
    "\n",
    "| Step | Action |\n",
    "|------|--------|\n",
    "| Load | Read insurance and claims data |\n",
    "| Aggregate | Count claims by (ID, year) |\n",
    "| Merge | Left join on (ID, year) |\n",
    "| Fill | NaN â†’ 0 for no claims |\n",
    "| Split | 80/20 train-test split |"
   ],
   "id": "e1024e657e86b56c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1 Load and merge data",
   "id": "790fcfa25c931bbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "insurance = pd.read_csv('../../data/input/Motor_vehicle_insurance_data.csv', delimiter=\";\")\n",
    "claims =  pd.read_csv('../../data/input/sample_type_claim.csv', delimiter=';')\n",
    "\n",
    "claims_frequency  = (\n",
    "    claims\n",
    "    .groupby(['ID', 'Cost_claims_year'])\n",
    "    .agg({'Cost_claims_by_type': 'count'})\n",
    "    .rename(columns={'Cost_claims_by_type': 'claims_frequency'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "dataset = (\n",
    "    pd\n",
    "    .merge(\n",
    "        left=insurance,\n",
    "        right=claims_frequency,\n",
    "        how='left',\n",
    "        on=['ID', 'Cost_claims_year']\n",
    "    )\n",
    "    .fillna(value={'claims_frequency':0})\n",
    ")\n",
    "\n",
    "trainset, testset = train_test_split(dataset, test_size=0.2, random_state=42, shuffle=True)"
   ],
   "id": "1f61d8c1a4df1d18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Engineer Relevant Features",
   "id": "d5b18254d6295948"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.model.freq_sev.feature import  main as feature_main\n",
    "features_trainset = feature_main(trainset)\n",
    "features_testset = feature_main(testset)"
   ],
   "id": "f1659ce83bdbf53d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Analyse Target Variable [on training data]\n",
    "\n",
    "The response variable is the number of claims dubbed `claims_frequency` in the dataset. It is important to note that Claims frequency is actually a rate (i.e. it is the claims frequency for a year for each policy-holder). However, since the exposure is constant for all policies in this dataset (1 year), it is an implicit rate.\n",
    "\n",
    "Let's check a few assumptions before we fit a Poisson regression model:\n",
    "1. Distribution of the response variable\n",
    "2. Equidispersion: the mean and variance of the response variable should be roughly equal"
   ],
   "id": "b5eba882a2ef7c02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Distribution of claims frequency",
   "id": "e488d3e63bdf41d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax0) = plt.subplots(ncols=1, figsize=(15, 5))\n",
    "ax0.set_title('Claims Frequency Distribution')\n",
    "_ = features_trainset['claims_frequency'].hist(bins=4, log=True, ax=ax0)\n",
    "\n",
    "print(\n",
    "    \"Average claims frequency: {}\".format(\n",
    "        np.average(features_trainset['claims_frequency'])\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Fraction of claims frequency that is Zero {0:.2%}\".format(\n",
    "        features_trainset[features_trainset['claims_frequency']==0].__len__() / features_trainset['claims_frequency'].__len__()\n",
    "    )\n",
    ")"
   ],
   "id": "35cfd3df35417e18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Check equidispersion assumption",
   "id": "e5b0c735e618b259"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mean_claims_frequency = features_trainset['claims_frequency'].mean()\n",
    "var_claims_frequency = features_trainset['claims_frequency'].var()\n",
    "print(f\"Mean of claims_frequency: {mean_claims_frequency :.4f}\")\n",
    "print(f\"Variance of claims_frequency: {var_claims_frequency:.4f}\")"
   ],
   "id": "8f417643e6d23c1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- From the both the histogram and the mean-variance comparison, we can see that the claims frequency is unimodal and rightly skewed, the mean and variance after filling nulls with 0 appear to be reasonably close for this dataset. We can therefore proceed to fit a Poisson regression model.",
   "id": "7cfc1b6d1b02f6c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Model Development\n",
    "\n",
    "| Model | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| Baseline | DummyRegressor | Predict mean |\n",
    "| Ridge | Linear | L2 regularization |\n",
    "| Poisson | GLM | Count-specific |\n",
    "| GBM | Ensemble | Non-linear |"
   ],
   "id": "d9111ce58fed2a16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.1 Define training variables and evaluation metrics",
   "id": "5fdffd4e81d45b1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_variables = ['Car_age_years', 'Type_risk', 'Area', 'Value_vehicle', 'Distribution_channel', 'Cylinder_capacity']\n",
    "target = ['claims_frequency']"
   ],
   "id": "3c6052db9f2e99d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_poisson_deviance,mean_squared_error\n",
    "\n",
    "def model_evaluation_metrics(estimator, df_test, target_variable=target, training_variables=training_variables):\n",
    "    y_pred = estimator.predict(df_test[training_variables])\n",
    "\n",
    "    print(\n",
    "        \"MSE: %.3f\"\n",
    "        % mean_squared_error(\n",
    "            df_test[target], y_pred,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"MAE: %.3f\"\n",
    "        % mean_absolute_error(\n",
    "            df_test[target], y_pred\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Ignore non-positive predictions, as they are invalid for\n",
    "    # the Poisson deviance.\n",
    "    mask = y_pred > 0\n",
    "    if (~mask).any():\n",
    "        n_masked, n_samples = (~mask).sum(), mask.shape[0]\n",
    "        print(\n",
    "            \"WARNING: Estimator yields invalid, non-positive predictions \"\n",
    "            f\" for {n_masked} samples out of {n_samples}. These predictions \"\n",
    "            \"are ignored when computing the Poisson deviance.\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"mean Poisson deviance: %.3f\"\n",
    "        % mean_poisson_deviance(\n",
    "            df_test[target][mask],\n",
    "            y_pred[mask],\n",
    "        )\n",
    "    )\n"
   ],
   "id": "e622d57c62602e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.2 Model 1: Baseline (Mean Prediction)",
   "id": "7937cd06b3196f7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "dummy_regressor = DummyRegressor(strategy=\"mean\")\n",
    "baseline_model = dummy_regressor.fit(features_trainset[training_variables], features_trainset[target])\n",
    "print(\"Constant mean frequency evaluation:\")\n",
    "model_evaluation_metrics(estimator=baseline_model, df_test=features_testset, target_variable=target, training_variables=training_variables)"
   ],
   "id": "f36b7fd099626782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3 Model 2: Ridge Regression",
   "id": "ae2f68a122f12964"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_glm = Ridge(alpha=1)\n",
    "ridge_model = ridge_glm.fit(features_trainset[training_variables], features_trainset[target])\n",
    "print(\"Ridge regression evaluation:\")\n",
    "model_evaluation_metrics(estimator=ridge_model, df_test=features_testset, target_variable=target, training_variables=training_variables)"
   ],
   "id": "fa87c6bf346fec9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.4 Model 3: Poisson Regression",
   "id": "2b559000e32b2d3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "poisson_regressor = PoissonRegressor(alpha=1e-12, solver='newton-cholesky', max_iter=300)\n",
    "poisson_model = poisson_regressor.fit(features_trainset[training_variables], features_trainset[target].values.ravel())\n",
    "print(\"Poisson regression evaluation:\")\n",
    "model_evaluation_metrics(estimator=poisson_model, df_test=features_testset, target_variable=target, training_variables=training_variables)"
   ],
   "id": "9203ec59923421c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.5 Model 4: Gradient Boosting Machine",
   "id": "8827cebe65f9316a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "gbm_regressor = HistGradientBoostingRegressor(loss='poisson', max_leaf_nodes=128)\n",
    "gbm_model = gbm_regressor.fit(features_trainset[training_variables], features_trainset['claims_frequency'])\n",
    "print(\"GBM regression evaluation:\")\n",
    "model_evaluation_metrics(estimator=gbm_model, df_test=features_testset, target_variable=target, training_variables=training_variables)"
   ],
   "id": "63d367062bc773b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "| Check | Purpose |\n",
    "|-------|---------|\n",
    "| Prediction distribution | Compare predicted vs actual |\n",
    "| Risk group analysis | Calibration across risk segments |"
   ],
   "id": "22eaf889955a2631"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.1 Prediction distribution comparison",
   "id": "ff07b0b97e0eccdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 6), sharey=True)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "n_bins = 20\n",
    "for row_idx, label, df in zip(range(2), [\"train\", \"test\"], [features_trainset, features_testset]):\n",
    "    df[\"claims_frequency\"].hist(bins=np.linspace(-1, 30, n_bins), ax=axes[row_idx, 0])\n",
    "\n",
    "    axes[row_idx, 0].set_title(\"Data\")\n",
    "    axes[row_idx, 0].set_yscale(\"log\")\n",
    "    axes[row_idx, 0].set_xlabel(\"y (observed Frequency)\")\n",
    "    axes[row_idx, 0].set_ylim([1e1, 5e5])\n",
    "    axes[row_idx, 0].set_ylabel(label + \" samples\")\n",
    "\n",
    "    for idx, model in enumerate([baseline_model, ridge_glm, poisson_regressor, gbm_regressor ]):\n",
    "        y_pred = model.predict(df[training_variables])\n",
    "\n",
    "        pd.Series(y_pred).hist(\n",
    "            bins=np.linspace(-1, 4, n_bins), ax=axes[row_idx, idx + 1]\n",
    "        )\n",
    "        axes[row_idx, idx + 1].set_title(model.__class__.__name__)\n",
    "        axes[row_idx, idx + 1].set_yscale(\"log\")\n",
    "        axes[row_idx, idx + 1].set_xlabel(\"y_pred (predicted expected Frequency)\")\n",
    "plt.tight_layout()"
   ],
   "id": "f47e526b18cc42d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 Risk group calibration",
   "id": "c4c59b71a4110f67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.utils import gen_even_slices\n",
    "\n",
    "\n",
    "def _mean_frequency_by_risk_group(y_true, y_pred, n_bins=100):\n",
    "    idx_sort = np.argsort(y_pred)\n",
    "    bin_centers = np.arange(0, 1, 1 / n_bins) + 0.5 / n_bins\n",
    "    y_pred_bin = np.zeros(n_bins)\n",
    "    y_true_bin = np.zeros(n_bins)\n",
    "\n",
    "    for n, sl in enumerate(gen_even_slices(len(y_true), n_bins)):\n",
    "        y_pred_bin[n] = np.average(y_pred[idx_sort][sl], )\n",
    "        y_true_bin[n] = np.average(y_true[idx_sort][sl], )\n",
    "    return bin_centers, y_true_bin, y_pred_bin\n",
    "\n",
    "\n",
    "print(f\"Actual number of claims: {features_testset['claims_frequency'].sum()}\")\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "for axi, model in zip(ax.ravel(), [baseline_model, ridge_glm, poisson_regressor, gbm_regressor]):\n",
    "    y_pred = model.predict(features_testset[training_variables])\n",
    "    y_true = features_testset['claims_frequency'].values\n",
    "    q, y_true_seg, y_pred_seg = _mean_frequency_by_risk_group(\n",
    "        y_true, y_pred, n_bins=10\n",
    "    )\n",
    "\n",
    "    print(f\"Predicted number of claims by {model.__class__.__name__}: {np.sum(y_pred):.1f}\")\n",
    "\n",
    "    axi.plot(q, y_pred_seg, marker=\"x\", linestyle=\"--\", label=\"predictions\")\n",
    "    axi.plot(q, y_true_seg, marker=\"o\", linestyle=\"--\", label=\"observations\")\n",
    "    axi.set_xlim(0, 1.0)\n",
    "    axi.set_ylim(0, 0.5)\n",
    "    axi.set_title(model.__class__.__name__)\n",
    "    axi.set_xlabel(\"Fraction of samples sorted by y_pred\")\n",
    "    axi.set_ylabel(\"Mean Frequency (y_pred)\")\n",
    "    axi.legend()\n",
    "plt.tight_layout()"
   ],
   "id": "b2b428ffcfef9c61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#TODO: Delete ridge, introduce mlflow",
   "id": "acf87c30e33ea08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Let's attempt to build a frequency vs severity model using both GLM and GBM. Remember that technically `premium = frequency * severity`\n",
    "- Bonus: see if along the way we can use the variable importance to identify key rating factors for both frequency and severity that can be used for risk classification"
   ],
   "id": "c32197f8b9577e32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "e87cdae9eedad556",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 001: Create the dataset and split dataset",
   "id": "9725c49a1631ccc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "insurance = pd.read_csv('../../data/input/Motor_vehicle_insurance_data.csv', delimiter=\";\")\n",
    "claims =  pd.read_csv('../../data/input/sample_type_claim.csv', delimiter=';')\n",
    "\n",
    "claims_frequency  = (\n",
    "    claims\n",
    "    .groupby(['ID', 'Cost_claims_year'])\n",
    "    .agg({'Cost_claims_by_type': 'count'})\n",
    "    .rename(columns={'Cost_claims_by_type': 'claims_frequency'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "dataset = (\n",
    "    pd\n",
    "    .merge(\n",
    "        left=insurance,\n",
    "        right=claims_frequency,\n",
    "        how='left',\n",
    "        on=['ID', 'Cost_claims_year']\n",
    "    )\n",
    "    .fillna(value={'claims_frequency':0})\n",
    ")\n",
    "\n",
    "trainset, testset = train_test_split(dataset, test_size=0.2, random_state=42, shuffle=True)"
   ],
   "id": "3e061f0528d7212b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 002: Engineer relevant features",
   "id": "b95a7817968c222d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.model.freq_sev.feature import  main as feature_main\n",
    "features_trainset = feature_main(trainset)\n",
    "features_testset = feature_main(testset)"
   ],
   "id": "7a22ff8fa3abcfc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 003: Frequency modelling - Poisson regression [on training data]\n",
    "\n",
    "The response variable is the number of claims dubbed `claims_frequency` in the dataset. It is important to note that Claims frequency is actually a rate (i.e. it is the claims frequency for a year for each policy-holder). However, since the exposure is constant for all policies in this dataset (1 year), it is an implicit rate.\n",
    "\n",
    "Let's check a few assumptions before we fit a Poisson regression model:\n",
    "1. Distribution of the response variable\n",
    "2. Equidispersion: the mean and variance of the response variable should be roughly equal"
   ],
   "id": "15ec8abc2192221d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax0) = plt.subplots(ncols=1, figsize=(15, 5))\n",
    "ax0.set_title('Claims Frequency Distribution')\n",
    "_ = features_trainset['claims_frequency'].hist(bins=4, log=True, ax=ax0)\n",
    "\n",
    "print(\n",
    "    \"Average claims frequency: {}\".format(\n",
    "        np.average(features_trainset['claims_frequency'])\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Fraction of claims frequency that is Zero {0:.2%}\".format(\n",
    "        features_trainset[features_trainset['claims_frequency']==0].__len__() / features_trainset['claims_frequency'].__len__()\n",
    "    )\n",
    ")"
   ],
   "id": "fccad599705c562f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mean_claims_frequency = features_trainset['claims_frequency'].mean()\n",
    "var_claims_frequency = features_trainset['claims_frequency'].var()\n",
    "print(f\"Mean of claims_frequency: {mean_claims_frequency :.4f}\")\n",
    "print(f\"Variance of claims_frequency: {var_claims_frequency:.4f}\")"
   ],
   "id": "535be0346e47222c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- From the both the histogram and the mean-variance comparison, we can see that the claims frequency is unimodal and rightly skewed, the mean and variance after filling nulls with 0 appear to be reasonably close for this dataset. We can therefore proceed to fit a Poisson regression model.",
   "id": "560bb6b71f815e1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_variables = ['Car_age_years', 'Type_risk', 'Area', 'Value_vehicle', 'Distribution_channel', 'Cylinder_capacity']\n",
    "target = ['claims_frequency']"
   ],
   "id": "a3753f3baa52c31d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_poisson_deviance,mean_squared_error\n",
    "\n",
    "def model_evaluation_metrics(estimator, df_test, target_variable=target, training_variables=training_variables):\n",
    "    y_pred = estimator.predict(df_test[training_variables])\n",
    "    print(\n",
    "        \"MSE: %.3f\"\n",
    "        % mean_squared_error(\n",
    "            df_test[target], y_pred,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"MAE: %.3f\"\n",
    "        % mean_absolute_error(\n",
    "            df_test[target], y_pred\n",
    "        )\n",
    "    )\n",
    "    mask = y_pred > 0\n",
    "    if (~mask).any():\n",
    "        n_masked, n_samples = (~mask).sum(), mask.shape[0]\n",
    "        print(\n",
    "            \"WARNING: Estimator yields invalid, non-positive predictions \"\n",
    "            f\" for {n_masked} samples out of {n_samples}. These predictions \"\n",
    "            \"are ignored when computing the Poisson deviance.\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"mean Poisson deviance: %.3f\"\n",
    "        % mean_poisson_deviance(\n",
    "            df_test[target][mask],\n",
    "            y_pred[mask],\n",
    "        )\n",
    "    )\n"
   ],
   "id": "cee61893a4d04cfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 1 - Baseline Model, Just predicting the mean",
   "id": "13ff805c0a11471b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "dummy_regressor = DummyRegressor(strategy=\"mean\")\n",
    "baseline_model = dummy_regressor.fit(features_trainset[training_variables], features_trainset[target])\n",
    "print(\"Constant mean frequency evaluation:\")\n",
    "model_evaluation_metrics(estimator=baseline_model, df_test=features_testset, target_variable=target, training_variables=training_variables)"
   ],
   "id": "f5b11aaa26eb0120",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 2 - Ridge Regression",
   "id": "518012b6c3233cff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_glm = Ridge(alpha=1)\n",
    "ridge_model = ridge_glm.fit(features_trainset[training_variables], features_trainset[target])\n",
    "print(\"Ridge regression evaluation:\")\n",
    "model_evaluation_metrics(estimator=ridge_model, df_test=features_testset, target_variable=target, training_variables=training_variables)"
   ],
   "id": "1dfbd2b1f3cb99ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 3 - Poisson Regression",
   "id": "97423099565f9906"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "poisson_regressor = PoissonRegressor(alpha=1e-12, solver='newton-cholesky', max_iter=300)\n",
    "poisson_model = poisson_regressor.fit(features_trainset[training_variables], features_trainset[target].values.ravel())\n",
    "print(\"Poisson regression evaluation:\")\n",
    "model_evaluation_metrics(estimator=poisson_model, df_test=features_testset, target_variable=target, training_variables=training_variables)"
   ],
   "id": "38746389a273d7d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 4 - Gradient Boosting Machine\n",
   "id": "d20ccabc6b1ff1f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "gbm_regressor = HistGradientBoostingRegressor(loss='poisson', max_leaf_nodes=128)\n",
    "gbm_model = gbm_regressor.fit(features_trainset[training_variables], features_trainset['claims_frequency'])\n",
    "print(\"GBM regression evaluation:\")\n",
    "model_evaluation_metrics(estimator=gbm_model, df_test=features_testset, target_variable=target, training_variables=training_variables)"
   ],
   "id": "f0cf8c877234993a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 004: Frequency Modelling Evaluation",
   "id": "86a6db5e21e7f97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 6), sharey=True)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "n_bins = 20\n",
    "for row_idx, label, df in zip(range(2), [\"train\", \"test\"], [features_trainset, features_testset]):\n",
    "    df[\"claims_frequency\"].hist(bins=np.linspace(-1, 30, n_bins), ax=axes[row_idx, 0])\n",
    "\n",
    "    axes[row_idx, 0].set_title(\"Data\")\n",
    "    axes[row_idx, 0].set_yscale(\"log\")\n",
    "    axes[row_idx, 0].set_xlabel(\"y (observed Frequency)\")\n",
    "    axes[row_idx, 0].set_ylim([1e1, 5e5])\n",
    "    axes[row_idx, 0].set_ylabel(label + \" samples\")\n",
    "\n",
    "    for idx, model in enumerate([baseline_model, ridge_glm, poisson_regressor, gbm_regressor ]):\n",
    "        y_pred = model.predict(df[training_variables])\n",
    "\n",
    "        pd.Series(y_pred).hist(\n",
    "            bins=np.linspace(-1, 4, n_bins), ax=axes[row_idx, idx + 1]\n",
    "        )\n",
    "        axes[row_idx, idx + 1].set_title(model.__class__.__name__)\n",
    "        axes[row_idx, idx + 1].set_yscale(\"log\")\n",
    "        axes[row_idx, idx + 1].set_xlabel(\"y_pred (predicted expected Frequency)\")\n",
    "plt.tight_layout()"
   ],
   "id": "95b2c343db3bc98f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.utils import gen_even_slices\n",
    "\n",
    "\n",
    "def _mean_frequency_by_risk_group(y_true, y_pred, n_bins=100):\n",
    "    idx_sort = np.argsort(y_pred)\n",
    "    bin_centers = np.arange(0, 1, 1 / n_bins) + 0.5 / n_bins\n",
    "    y_pred_bin = np.zeros(n_bins)\n",
    "    y_true_bin = np.zeros(n_bins)\n",
    "\n",
    "    for n, sl in enumerate(gen_even_slices(len(y_true), n_bins)):\n",
    "        y_pred_bin[n] = np.average(y_pred[idx_sort][sl], )\n",
    "        y_true_bin[n] = np.average(y_true[idx_sort][sl], )\n",
    "    return bin_centers, y_true_bin, y_pred_bin\n",
    "\n",
    "\n",
    "print(f\"Actual number of claims: {features_testset['claims_frequency'].sum()}\")\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "for axi, model in zip(ax.ravel(), [baseline_model, ridge_glm, poisson_regressor, gbm_regressor]):\n",
    "    y_pred = model.predict(features_testset[training_variables])\n",
    "    y_true = features_testset['claims_frequency'].values\n",
    "    q, y_true_seg, y_pred_seg = _mean_frequency_by_risk_group(\n",
    "        y_true, y_pred, n_bins=10\n",
    "    )\n",
    "\n",
    "    print(f\"Predicted number of claims by {model.__class__.__name__}: {np.sum(y_pred):.1f}\")\n",
    "\n",
    "    axi.plot(q, y_pred_seg, marker=\"x\", linestyle=\"--\", label=\"predictions\")\n",
    "    axi.plot(q, y_true_seg, marker=\"o\", linestyle=\"--\", label=\"observations\")\n",
    "    axi.set_xlim(0, 1.0)\n",
    "    axi.set_ylim(0, 0.5)\n",
    "    axi.set_title(model.__class__.__name__)\n",
    "    axi.set_xlabel(\"Fraction of samples sorted by y_pred\")\n",
    "    axi.set_ylabel(\"Mean Frequency (y_pred)\")\n",
    "    axi.legend()\n",
    "plt.tight_layout()"
   ],
   "id": "76d56f901dfbc171",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
